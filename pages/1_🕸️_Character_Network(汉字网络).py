import streamlit as st
import pandas as pd
import plotly.express as px
from utils import page_header, load_data
from pyvis.network import Network
import networkx as nx
import os
import streamlit.components.v1 as components
import json

# ----------------------------
# Page Configuration
# ----------------------------
st.set_page_config(layout="wide")

# ----------------------------
# Translation Strings
# ----------------------------
translations = {
    'en': {
        'page_title': "Character Hub",
        'load_error': "Data could not be loaded. Please ensure the data file is available.",
        'settings_header': "âš™ï¸ Settings",
        'language_select': "Select Language",
        'controls_header': "ðŸ” Controls",
        'filter_by_class': "Filter by Verb Class",
        'highlight_char': "Select Character to Analyze",
        'no_match_warning': "No data to display for the current selection.",
        'network_header': "Interactive Character Network",
        'network_desc': """
        This graph visualizes how characters form verbs. Use the sidebar to **filter by verb class**.
        - **Nodes:** Single Chinese characters, colored by class.
        - **Edges:** An arrow indicates a verb is formed (e.g., A â†’ B means the verb is 'AB').
        """,
        'generating_network': "Generating network graph...",
        'char_stats_header': "Character Statistics Explorer",
        'select_char_prompt': "Please select a character to see its statistics.",
        'starts_verbs_metric': "Starts Verbs",
        'ends_verbs_metric': "Ends Verbs",
        'total_verbs_metric': "Total Connections",
        'verbs_list_expander': "Show list of verbs containing this character",
        'tab_network': "ðŸŒ Network Graph",
        'tab_pathways': "ðŸ“– Learning Pathways",
        'tab_families': "ðŸ§© Word Families",
        'tab_stats': "ðŸ”¢ Character Statistics",
        'learning_pathways_header': "Data-Driven Learning Pathways",
        'learning_pathways_desc': "Use network science to find the most important characters to learn first. Analysis uses the current class filter.",
        'centrality_expander': "ðŸ”‘ Most Connected Characters (Degree Centrality)",
        'centrality_desc': "**Why it matters:** These are 'super-connector' characters. Learning them first helps you recognize and form the largest number of verbs quickly.",
        'betweenness_expander': "ðŸŒ‰ Key Bridging Characters (Betweenness Centrality)",
        'betweenness_desc': "**Why it matters:** These characters act as bridges connecting different groups of words (word families). Mastering them helps link different vocabulary sets together.",
        'character_col': "Character",
        'score_col': "Score",
        'in_degree_col': "Ends",
        'out_degree_col': "Starts",
        'families_header': "Word Family Explorer",
        'families_desc': "Using community detection algorithms, we can find clusters of characters that are highly interconnected. Learning these 'word families' together can be an effective strategy.",
        'family_select': "Select a Word Family to Explore",
        'family_members': "Family Members",
        'family_verbs_header': "Verbs within this Family",
        'family_graph_header': "Family Network Graph",
        'family_label': "Family",
    },
    'zh': {
        'page_title': "æ±‰å­—ä¸­å¿ƒ",
        'load_error': "æ— æ³•åŠ è½½æ•°æ®ã€‚è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å¯ç”¨ã€‚",
        'settings_header': "âš™ï¸ è®¾ç½®",
        'language_select': "é€‰æ‹©è¯­è¨€",
        'controls_header': "ðŸ” æŽ§åˆ¶é¢æ¿",
        'filter_by_class': "æŒ‰åŠ¨è¯ç±»åˆ«ç­›é€‰",
        'highlight_char': "é€‰æ‹©è¦åˆ†æžçš„æ±‰å­—",
        'no_match_warning': "æ²¡æœ‰ç¬¦åˆå½“å‰ç­›é€‰æ¡ä»¶çš„æ•°æ®ã€‚",
        'network_header': "äº’åŠ¨æ±‰å­—ç½‘ç»œ",
        'network_desc': """
        æ­¤å›¾å±•ç¤ºæ±‰å­—å¦‚ä½•æž„æˆåŠ¨è¯ã€‚è¯·ä½¿ç”¨ä¾§æ æŒ‰**åŠ¨è¯ç±»åˆ«**è¿›è¡Œç­›é€‰ã€‚
        - **èŠ‚ç‚¹ï¼š** å•ä¸ªæ±‰å­—ï¼ŒæŒ‰ç±»åˆ«ç€è‰²ã€‚
        - **è¾¹ï¼š** ç®­å¤´è¡¨ç¤ºæž„æˆä¸€ä¸ªåŠ¨è¯ï¼ˆä¾‹å¦‚ A â†’ B è¡¨ç¤ºâ€œABâ€ï¼‰ã€‚
        """,
        'generating_network': "æ­£åœ¨ç”Ÿæˆç½‘ç»œå›¾...",
        'char_stats_header': "æ±‰å­—ç»Ÿè®¡æµè§ˆå™¨",
        'select_char_prompt': "è¯·é€‰æ‹©ä¸€ä¸ªæ±‰å­—ä»¥æŸ¥çœ‹å…¶ç»Ÿè®¡æ•°æ®ã€‚",
        'starts_verbs_metric': "ä½œä¸ºé¦–å­—",
        'ends_verbs_metric': "ä½œä¸ºå°¾å­—",
        'total_verbs_metric': "æ€»è¿žæŽ¥æ•°",
        'verbs_list_expander': "æ˜¾ç¤ºåŒ…å«æ­¤æ±‰å­—çš„åŠ¨è¯åˆ—è¡¨",
        'tab_network': "ðŸŒ ç½‘ç»œå›¾",
        'tab_pathways': "ðŸ“– å­¦ä¹ è·¯å¾„",
        'tab_families': "ðŸ§© è¯æ—",
        'tab_stats': "ðŸ”¢ æ±‰å­—ç»Ÿè®¡",
        'learning_pathways_header': "æ•°æ®é©±åŠ¨çš„å­¦ä¹ è·¯å¾„",
        'learning_pathways_desc': "åˆ©ç”¨ç½‘ç»œç§‘å­¦æ‰¾å‡ºæœ€é‡è¦çš„æ±‰å­—ï¼Œä¼˜å…ˆå­¦ä¹ ã€‚åˆ†æžåŸºäºŽå½“å‰çš„ç±»åˆ«ç­›é€‰ã€‚",
        'centrality_expander': "ðŸ”‘ è¿žæŽ¥æœ€å¤šçš„æ ¸å¿ƒå­—ï¼ˆåº¦ä¸­å¿ƒæ€§ï¼‰",
        'centrality_desc': "**é‡è¦æ€§ï¼š** è¿™äº›æ˜¯â€œè¶…çº§è¿žæŽ¥è¯â€ã€‚ä¼˜å…ˆå­¦ä¹ å®ƒä»¬æœ‰åŠ©äºŽå¿«é€Ÿè¯†åˆ«å’Œæž„æˆæ›´å¤šåŠ¨è¯ã€‚",
        'betweenness_expander': "ðŸŒ‰ å…³é”®æ¡¥æ¢å­—ï¼ˆä¸­ä»‹ä¸­å¿ƒæ€§ï¼‰",
        'betweenness_desc': "**é‡è¦æ€§ï¼š** è¿™äº›æ±‰å­—å¦‚æ¡¥æ¢ï¼Œè¿žæŽ¥ä¸åŒè¯æ—ã€‚æŽŒæ¡å®ƒä»¬æœ‰åŠ©äºŽæŠŠä¸åŒè¯æ±‡é›†è”ç³»åœ¨ä¸€èµ·ã€‚",
        'character_col': "æ±‰å­—",
        'score_col': "å¾—åˆ†",
        'in_degree_col': "ä½œå°¾å­—æ¬¡æ•°",
        'out_degree_col': "ä½œé¦–å­—æ¬¡æ•°",
        'families_header': "è¯æ—æµè§ˆå™¨",
        'families_desc': "é€šè¿‡ç¤¾ç¾¤æ£€æµ‹ç®—æ³•ï¼Œæˆ‘ä»¬å¯ä»¥å‘çŽ°å†…éƒ¨è”ç³»ç´§å¯†çš„æ±‰å­—é›†ç¾¤ã€‚å°†è¿™äº›â€œè¯æ—â€ä¸€èµ·å­¦ä¹ å¯èƒ½æ˜¯ä¸€ç§æœ‰æ•ˆç­–ç•¥ã€‚",
        'family_select': "é€‰æ‹©ä¸€ä¸ªè¯æ—è¿›è¡ŒæŽ¢ç´¢",
        'family_members': "è¯æ—æˆå‘˜",
        'family_verbs_header': "è¯¥è¯æ—å†…çš„åŠ¨è¯",
        'family_graph_header': "è¯æ—ç½‘ç»œå›¾",
        'family_label': "è¯æ—",
    }
}

# ----------------------------
# Caching Functions
# ----------------------------
@st.cache_data
def build_graph(df):
    G = nx.DiGraph()
    for _, row in df.iterrows():
        if pd.notna(row['char1']) and pd.notna(row['char2']):
            G.add_edge(row['char1'], row['char2'], title=f"{row['Verb']} ({row['pinyin']})")
    return G

@st.cache_data
def get_communities(_G):
    G_undirected = _G.to_undirected()
    communities = nx.community.greedy_modularity_communities(G_undirected)
    return sorted([list(c) for c in communities], key=len, reverse=True)

# ----------------------------
# Sidebar
# ----------------------------
st.sidebar.header("âš™ï¸ Settings / è®¾ç½®")
lang_options = {'English': 'en', 'ä¸­æ–‡ (Chinese)': 'zh'}
selected_lang_display = st.sidebar.radio("Select Language / é€‰æ‹©è¯­è¨€", options=lang_options.keys(), horizontal=True)
lang = lang_options[selected_lang_display]
T = translations[lang]

page_header(T['page_title'], "ðŸ•¸ï¸")

# --- Data Loading and Processing ---
@st.cache_data
def get_df():
    return load_data()
df = get_df()

if df.empty:
    st.error(T['load_error'])
    st.stop()

def parse_bilingual(text):
    if isinstance(text, str) and '(' in text and ')' in text:
        parts = text.split('(')
        zh, en = parts[0], parts[1].replace(')', '')
        return zh.strip(), en.strip()
    return text, text

# Bilingual classification
df[['Classification_zh', 'Classification_en']] = df['åˆ†ç±»ï¼ˆClassificationï¼‰'].apply(lambda x: pd.Series(parse_bilingual(x)))
df.rename(columns={'Chinese_Verbs': 'Verb'}, inplace=True)
classification_col_display = 'Classification_zh' if lang == 'zh' else 'Classification_en'

# --- Sidebar Filters (by class) ---
st.sidebar.header(T['controls_header'])
unique_classes = sorted(df[classification_col_display].dropna().unique())
selected_classes = st.sidebar.multiselect(T['filter_by_class'], options=unique_classes, default=unique_classes)

# Filter data by selected classes
filtered_df = df[df[classification_col_display].isin(selected_classes)].copy()
G = build_graph(filtered_df)

# ----------------------------
# Main Content Tabs
# ----------------------------
tab1, tab2, tab3, tab4 = st.tabs([T['tab_network'], T['tab_pathways'], T['tab_families'], T['tab_stats']])

# ----------------------------
# TAB 1 â€“ Network Graph
# ----------------------------
with tab1:
    st.header(T['network_header'])
    st.markdown(T['network_desc'])

    if not filtered_df.empty:
        with st.spinner(T['generating_network']):
            # Compute node sizes from degree on filtered graph
            degrees = dict(G.degree())
            min_degree, max_degree = (1, 1)
            if degrees:
                min_degree = min(degrees.values())
                max_degree = max(degrees.values())

            if max_degree == min_degree:
                normalized_degrees = {node: 15 for node in degrees}
            else:
                normalized_degrees = {
                    node: 10 + 25 * (deg - min_degree) / (max_degree - min_degree)
                    for node, deg in degrees.items()
                }

            net = Network(
                height='750px', width='100%', notebook=False, directed=True,
                cdn_resources='in_line', select_menu=True, filter_menu=True
            )

            # Add nodes, colored/grouped by class (from filtered data, fallback to full df)
            for node, size in normalized_degrees.items():
                # try filtered_df first; if node not present (edge case), fallback to df
                pool = filtered_df if ((filtered_df['char1'] == node) | (filtered_df['char2'] == node)).any() else df
                classification = pool.loc[(pool['char1'] == node) | (pool['char2'] == node), classification_col_display].iloc[0]
                net.add_node(node, label=node, size=size, font={'size': size + 10}, group=classification)

            # Add edges for filtered set
            for _, row in filtered_df.iterrows():
                if row['char1'] and row['char2']:
                    net.add_edge(row['char1'], row['char2'], title=row['Verb'])

            try:
                file_path = 'verb_network.html'
                net.save_graph(file_path)
                with open(file_path, 'r', encoding='utf-8') as f:
                    source_code = f.read()
                st.components.v1.html(source_code, height=800)
                os.remove(file_path)
            except Exception as e:
                st.error(f"Error displaying network graph: {e}")
    else:
        st.warning(T['no_match_warning'])

# ----------------------------
# TAB 2 â€“ Learning Pathways
# ----------------------------
with tab2:
    st.header(T['learning_pathways_header'])
    st.markdown(T['learning_pathways_desc'])

    if len(G.nodes) > 1:
        col1, col2 = st.columns(2)
        in_degree = dict(G.in_degree())
        out_degree = dict(G.out_degree())
        
        with col1:
            with st.expander(T['centrality_expander'], expanded=True):
                st.markdown(T['centrality_desc'])
                degree_cent = nx.degree_centrality(G)
                top_degree = sorted(degree_cent.items(), key=lambda x: -x[1])[:10]
                df_degree = pd.DataFrame(top_degree, columns=[T['character_col'], T['score_col']])
                df_degree[T['in_degree_col']] = df_degree[T['character_col']].map(in_degree)
                df_degree[T['out_degree_col']] = df_degree[T['character_col']].map(out_degree)
                df_degree[T['score_col']] = df_degree[T['score_col']].round(3)
                
                fig = px.bar(df_degree, x=T['score_col'], y=T['character_col'], orientation='h', text_auto=True)
                fig.update_layout(yaxis={'categoryorder':'total ascending'})
                st.plotly_chart(fig, use_container_width=True)
                st.dataframe(df_degree, use_container_width=True)

        with col2:
            with st.expander(T['betweenness_expander'], expanded=True):
                st.markdown(T['betweenness_desc'])
                between_cent = nx.betweenness_centrality(G)
                top_between = sorted(between_cent.items(), key=lambda x: -x[1])[:10]
                df_between = pd.DataFrame(top_between, columns=[T['character_col'], T['score_col']])
                df_between[T['in_degree_col']] = df_between[T['character_col']].map(in_degree)
                df_between[T['out_degree_col']] = df_between[T['character_col']].map(out_degree)
                df_between[T['score_col']] = df_between[T['score_col']].round(3)
               
                fig = px.bar(df_between, x=T['score_col'], y=T['character_col'], orientation='h', text_auto=True)
                fig.update_layout(yaxis={'categoryorder':'total ascending'})
                st.plotly_chart(fig, use_container_width=True)
                st.dataframe(df_between, use_container_width=True)
    else:
        st.warning(T['no_match_warning'])

# ----------------------------
# TAB 3 â€“ Word Families
# ----------------------------
with tab3:
    st.header(T['families_header'])
    st.markdown(T['families_desc'])
    
    if len(G.nodes) > 1:
        communities = [c for c in get_communities(G) if len(c) > 2][:20]
        if communities:
            fam_options = {f"{T['family_label']} {i+1} ({len(c)} {T['character_col']}s)": c for i, c in enumerate(communities)}
            selected_fam_label = st.selectbox(T['family_select'], options=fam_options.keys())
            
            if selected_fam_label:
                selected_community = fam_options[selected_fam_label]
                st.info(f"**{T['family_members']}:** {', '.join(selected_community)}")
                
                community_verbs_df = filtered_df[filtered_df['char1'].isin(selected_community) & filtered_df['char2'].isin(selected_community)]
                
                st.subheader(T['family_graph_header'])
                if not community_verbs_df.empty:
                    C_graph = build_graph(community_verbs_df)
                    net_fam = Network(height='700px', width='100%', notebook=False, directed=True, cdn_resources='in_line')
                    for node in C_graph.nodes():
                        net_fam.add_node(node, label=node, size=10 + 3*C_graph.degree(node), font={'size': 18})
                    for edge in C_graph.edges(data=True):
                        net_fam.add_edge(edge[0], edge[1], title=edge[2]['title'])
                    
                    try:
                        file_path_fam = 'family_network.html'
                        net_fam.save_graph(file_path_fam)
                        with open(file_path_fam, 'r', encoding='utf-8') as f:
                            source_code_fam = f.read()
                        components.html(source_code_fam, height=550)
                        if os.path.exists(file_path_fam):
                            os.remove(file_path_fam)
                    except Exception as e:
                        st.error(f"Error displaying graph: {e}")
                
                st.subheader(T['family_verbs_header'])
                st.dataframe(community_verbs_df[['Verb', 'pinyin', 'English_Verb', classification_col_display]], use_container_width=True)
        else:
            st.warning(T['no_match_warning'])
    else:
        st.warning(T['no_match_warning'])

# ----------------------------
# TAB 4 â€“ Character Statistics
# ----------------------------
with tab4:
    st.header(T['char_stats_header'])
    
    all_chars = sorted(list(G.nodes()))
    selected_char = st.selectbox(T['highlight_char'], options=[''] + all_chars)

    if selected_char and selected_char in G:
        st.subheader(f"'{selected_char}'")
        col1, col2, col3 = st.columns(3)
        col1.metric(T['starts_verbs_metric'], G.out_degree(selected_char))
        col2.metric(T['ends_verbs_metric'], G.in_degree(selected_char))
        col3.metric(T['total_verbs_metric'], G.degree(selected_char))
        
        with st.expander(T['verbs_list_expander']):
            st.dataframe(
                filtered_df[
                    (filtered_df['char1'] == selected_char) | (filtered_df['char2'] == selected_char)
                ][['Verb', 'pinyin', 'English_Verb', classification_col_display]].drop_duplicates(),
                use_container_width=True
            )
    else:
        st.info(T['select_char_prompt'])
